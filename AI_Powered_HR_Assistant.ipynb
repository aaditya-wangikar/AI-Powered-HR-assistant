{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ca02f4-3e9c-42c0-bbe3-d61d5cd97dab",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71402903-1061-4106-a483-b0282d407b4a",
   "metadata": {},
   "source": [
    "The project aims to create a conversational chatbot that responds to user inquiries using PDF document information. It requires proficiency in extracting and converting text into numerical vectors, establishing an answer-finding mechanism, and designing a user-friendly chatbot interface with Gradio. Additionally, the initiative emphasizes structuring inquiries for clear communication and deploying the chatbot for practical use, guaranteeing the system's accessibility and efficiency in meeting user needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3734549-83cf-48c3-a943-e7be690440a0",
   "metadata": {},
   "source": [
    "As a developer, you have received the critical task of improving the operational efficiency of Nestlé's human resources department, a leading multinational corporation. Your toolkit includes cutting-edge conversational AI technology, Python libraries, the powerful GPT model from OpenAI, and the user-friendly Gradio UI. Your mission is to integrate these advanced tools seamlessly to transform HR processes, creating a more streamlined and efficient workflow within the Nestlé organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb637ab-cfe8-4ad7-8238-7c0e3719ae5b",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708c7f5-685e-4bf4-aedc-b4792d060a31",
   "metadata": {},
   "source": [
    "Your task is to develop a conversational chatbot. This chatbot must answer queries about Nestlé's HR reports efficiently. Use Python libraries, OpenAI's GPT model, and Gradio UI. These tools will help you create a user-friendly interface. This interface will extract and process information from documents. It will provide accurate responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae582db6-8310-4c1a-97d7-f1872d08e915",
   "metadata": {},
   "source": [
    "# Description\n",
    "This script creates a Retrieval-Augmented Generation (RAG) chatbot that answers questions about the Nestlé HR Policy PDF.\n",
    "1. Reads OpenAI API key from a text file.\n",
    "2. Loads the Nestle HR PDF.\n",
    "3. Cleans and chunks the text for embedding.\n",
    "4. Builds a vector database (Chroma) for semantic search.\n",
    "5. Uses OpenAI embeddings to encode text.\n",
    "6. Retrieves relevant chunks when a user asks a question.\n",
    "7. Sends the retrieved context and user question to OpenAI LLM.\n",
    "8. Wraps everything in a Gradio Chat UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fcf77-ada2-43c9-a474-399b86122845",
   "metadata": {},
   "source": [
    "**Import libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "49896b8c-a129-42a3-969e-91cd5ae9a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a47432-1880-4bf0-a0f4-f500d0ef861b",
   "metadata": {},
   "source": [
    "**Read API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "836da331-9704-4c92-b5d9-874789c5142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "KEY_FILE = \"API_KEY.txt\"\n",
    "\n",
    "if not os.path.exists(KEY_FILE):\n",
    "    raise FileNotFoundError(\"API_KEY.txt not found. Create it and put your OpenAI key inside.\")\n",
    "\n",
    "with open(KEY_FILE, \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API_KEY.txt is empty — add your OpenAI key.\")\n",
    "\n",
    "# Make key available to OpenAI + LangChain embeddings\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# Initialize official OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"OpenAI API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3a692-a7ef-438a-bb60-3e25d641c989",
   "metadata": {},
   "source": [
    "**Load and split document**\n",
    "\n",
    "Loads the PDF page by page, extracts text, prints the first paragraph for debugging, and returns full extracted text.\n",
    "- Helps confirm PDF is correctly loaded & parsed.\n",
    "- Shows the content of the PDF.\n",
    "- Ensures meaningful text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "dac9c6e8-7e4a-432e-ab51-2f8a0cbdd396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- FIRST PARAGRAPH FROM PDF ----------\n",
      "\n",
      "Policy\n",
      "Mandatory\n",
      "September  2012\n",
      "The Nestlé  \n",
      "Human Resources Policy\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "PDF Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "PDF_PATH = \"Dataset/the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    raise FileNotFoundError(f\"PDF not found at {PDF_PATH}\")\n",
    "\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    full_text = \"\"\n",
    "    first_paragraph = None\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            full_text += text + \"\\n\"\n",
    "\n",
    "            # Grab first paragraph\n",
    "            if first_paragraph is None:\n",
    "                parts = text.strip().split(\"\\n\\n\")\n",
    "                parts = [p.strip() for p in parts if p.strip()]\n",
    "                if len(parts) > 0:\n",
    "                    first_paragraph = parts[0]\n",
    "\n",
    "    if first_paragraph is None:\n",
    "        first_paragraph = \"No readable paragraph found.\"\n",
    "\n",
    "    print(\"\\n---------- FIRST PARAGRAPH FROM PDF ----------\\n\")\n",
    "    print(first_paragraph)\n",
    "    print(\"\\n----------------------------------------------\\n\")\n",
    "\n",
    "    return full_text\n",
    "\n",
    "pdf_text = load_pdf(PDF_PATH)\n",
    "print(\"PDF Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e00a3-d3e6-4c0a-a706-61830960f15a",
   "metadata": {},
   "source": [
    "**Chunk the text for RAG**\n",
    "\n",
    "Splits text into overlapping chunks for embedding. Required because\n",
    "- Embeddings have token limits.\n",
    "- Smaller chunks improve retrieval accuracy.\n",
    "- Overlap ensures no important sentence boundaries are lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5401a681-0259-4872-aeed-a42d98647fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks Created: 24\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(pdf_text)\n",
    "print(\"Total Chunks Created:\", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529612f-6f3c-4a19-ba73-ce5107a0c078",
   "metadata": {},
   "source": [
    "**Build vector database using chromadb and openai embeddings**\n",
    "\n",
    "Retrieves the top-k most relevant chunks. Uses similarity_search which is the most stable API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "96c0c427-c6a8-4751-a861-3cacef86f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"nestle_hr_data\"\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Vector DB created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26645558-7958-4549-8ef2-36a72b0b9a20",
   "metadata": {},
   "source": [
    "**Rag pipeline function**\n",
    "1. Retrieve top K chunks from vector DB\n",
    "2. Build a context-aware prompt\n",
    "3. Call OpenAI\n",
    "4. Return the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "7e598103-f9bd-4fc7-916d-8b10a2cee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    try:\n",
    "        if isinstance(question, tuple):\n",
    "            question = question[0]\n",
    "\n",
    "        # Retrieve top chunks\n",
    "        docs = retriever.invoke(question)\n",
    "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an HR chatbot for Nestlé.\n",
    "        Use the context below to answer.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Call OpenAI \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=400,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f43773-3bf1-4103-b3d0-f28cfcac2679",
   "metadata": {},
   "source": [
    "Test LLM with sample test question to check if it works without gradio (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "43fb864a-8c51-4b92-9b66-870163cf07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RAG pipeline without Gradio...\n",
      "\n",
      "Retrieved docs: 4\n",
      "\n",
      "Context extracted successfully.\n",
      "\n",
      "Prompt prepared.\n",
      "\n",
      "\n",
      "LLM Response:\n",
      "\n",
      "- The document outlines the Nestlé employee lifecycle.\n",
      "- It emphasizes the Nestlé Management and Leadership Principles, which guide employee actions and interactions.\n",
      "- It refers to the Corporate Business Principles that Nestlé follows globally.\n",
      "- The document serves as the foundation for Nestlé's policies.\n",
      "- Implementation is based on sound judgment, local laws, and common sense.\n",
      "- It promotes the idea that \"At Nestlé we put people at the centre of everything we do.\"\n",
      "- Authored by Jean-Marc Duvoisin, Deputy Executive Vice President.\n"
     ]
    }
   ],
   "source": [
    "# TEST LLM CALL WITHOUT GRADIO\n",
    "\n",
    "test_question = \"Explain the document?\"\n",
    "\n",
    "print(\"\\nRunning RAG pipeline without Gradio...\")\n",
    "\n",
    "try:\n",
    "    # Retrieve documents\n",
    "    docs = retriever.invoke(test_question)\n",
    "    print(\"\\nRetrieved docs:\", len(docs))\n",
    "\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    print(\"\\nContext extracted successfully.\\n\")\n",
    "\n",
    "    # Build prompt\n",
    "    prompt_text = f\"\"\"\n",
    "    You are an HR chatbot for Nestlé. Always give answers in points\n",
    "    Use ONLY the context below to answer.\n",
    "\n",
    "    If answer not found, say:\n",
    "    \"Information not found in the document.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {test_question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Prompt prepared.\\n\")\n",
    "\n",
    "    # Call OpenAI API directly\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    print(\"\\nLLM Response:\\n\")\n",
    "    print(answer)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR OCCURRED:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b705c7-6f32-45bf-8a22-3377634144de",
   "metadata": {},
   "source": [
    "**Deploy chatbot using gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "58421de2-8cd0-45c1-bd7a-5835de3b0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def respond(message, history):\n",
    "    try:\n",
    "        answer = generate_answer(message)\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {e}\"\n",
    "\n",
    "    return answer\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Nestle HR Chatbot\",\n",
    "    examples=[\"Explain the document\", \"What are the contents?\"],\n",
    "    description=\"Gradio interface for Nestle HT Chatbot\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb356b90-06cb-4fc7-a95b-e7226a32ac44",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The workflow begins by securely loading the OpenAI API key and extracting text from the Nestle HR Policy PDF. The extracted text is then divided into overlapping chunks, which are embedded using OpenAI’s embedding model and stored in a Chroma vector database for efficient semantic search. When a user asks a question, the system retrieves the most relevant text chunks from the database and combines them with the ongoing conversation history to construct a rich, context-aware prompt. This prompt is sent to the GPT-3.5-Turbo model, which generates a focused answer based solely on the retrieved content. A  Gradio ChatInterface wraps the entire pipeline, ensuring that only the user’s current message is passed to the backend, while memory is managed separately to support natural multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae9cfc-6b71-4048-b207-5d726e5bea71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "nestle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
